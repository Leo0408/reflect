{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REFLECT 持续学习与适应（Failure -> Correction -> Learning）集成演示\n",
        "\n",
        "本 Notebook 将使用 **real-world 模块和真实数据集**，演示如何接入\"本地专家模型 + 决策融合 + 失败-纠正-学习\"的自适应框架。\n",
        "\n",
        "你将看到：\n",
        "- 环境设置：加载 real-world 模块和真实数据集（`reflect_dataset/real_data`）\n",
        "- 任务配置：选择一个真实任务（如 `makeCoffee2`）\n",
        "- 场景图生成：使用 real-world 的 `get_scene_graph` 生成场景描述（作为 V-LLM 描述）\n",
        "- REFLECT 失败检测：使用 LLMPrompter 进行任务成功/失败判定\n",
        "- 创建自适应执行器：集成本地专家模型和决策融合\n",
        "- 第一次执行：预计失败并记录到数据库\n",
        "- 人类纠正：模拟人类标注，让本地专家学习\n",
        "- 再次执行：验证\"不要在同一地方摔倒两次\"的效果\n",
        "\n",
        "**本 Notebook 使用真实数据，无需占位实现！**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 1: 环境设置和依赖导入\n",
        "\n",
        "本步骤将：\n",
        "- 设置工作目录到 `real-world`\n",
        "- 导入必要的库和模块\n",
        "- 配置离线环境变量（避免在线下载）\n",
        "\n",
        "**注意**：如果遇到 `roberta-base` tokenizer 加载错误：\n",
        "- 方案1：确保本地有 `roberta-base` 模型（在 `{repo_root}/models/roberta-base`）\n",
        "- 方案2：临时关闭离线模式（注释掉 `TRANSFORMERS_OFFLINE=1`），允许从缓存或网络下载\n",
        "- 方案3：如果不需要实时检测，可以跳过 MDETR 初始化（使用已有检测结果）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "当前工作目录: /home/fdse/zzy/reflect/real-world\n"
          ]
        }
      ],
      "source": [
        "# 切换到 real-world 目录\n",
        "import os\n",
        "repo_root = \"/home/fdse/zzy\"\n",
        "os.chdir(f\"{repo_root}/reflect/real-world\")\n",
        "print(f\"当前工作目录: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 创建了 LLM/__init__.py 文件\n",
            "基础库导入成功！\n"
          ]
        }
      ],
      "source": [
        "# 导入基础库\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import zarr\n",
        "from pathlib import Path\n",
        "\n",
        "# 设置 matplotlib 中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 确保 LLM 目录被识别为 Python 包（如果缺少 __init__.py）\n",
        "llm_init_file = Path(\"LLM/__init__.py\")\n",
        "if not llm_init_file.exists():\n",
        "    llm_init_file.touch()\n",
        "    print(\"✓ 创建了 LLM/__init__.py 文件\")\n",
        "\n",
        "print(\"基础库导入成功！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  本地 Roberta 模型不存在: /home/fdse/zzy/models/roberta-base\n",
            "   将尝试使用 HuggingFace 缓存（如果离线模式失败，请先下载模型）\n",
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fdse/anaconda3/envs/reflect_env/lib/python3.9/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhubconf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mdetr_efficientnetB3_phrasecut\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_scene_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SceneGraph, Node \u001b[38;5;28;01mas\u001b[39;00m SceneGraphNode\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_get_local_sg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_scene_graph\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_robot_plan\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLLM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMPrompter\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/real_world_get_local_sg.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mo3d\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from detic_object_detector import *\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmdetr_object_detector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_scene_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m depth_to_point_cloud\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/mdetr_object_detector.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model = mdetr_efficientnetB5(pretrained=True).to(device)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m seg_model \u001b[38;5;241m=\u001b[39m \u001b[43mmdetr_efficientnetB3_phrasecut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m seg_model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/hubconf.py:211\u001b[0m, in \u001b[0;36mmdetr_efficientnetB3_phrasecut\u001b[0;34m(pretrained, threshold, return_postprocessor)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmdetr_efficientnetB3_phrasecut\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, return_postprocessor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    MDETR ENB3 with 6 encoder and 6 decoder layers.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Trained on Phrasecut, achieves 53.7 M-IoU on the test set\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_make_detr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimm_tf_efficientnet_b3_ns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_align_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m    213\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m    214\u001b[0m             url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://zenodo.org/record/4721981/files/phrasecut_EB3_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m             map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m             check_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    217\u001b[0m         )\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/hubconf.py:43\u001b[0m, in \u001b[0;36m_make_detr\u001b[0;34m(backbone_name, num_queries, mask, qa_dataset, predict_final, text_encoder, contrastive_align_loss)\u001b[0m\n\u001b[1;32m     41\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     42\u001b[0m backbone \u001b[38;5;241m=\u001b[39m _make_backbone(backbone_name, mask)\n\u001b[0;32m---> 43\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_intermediate_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_encoder_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m detr \u001b[38;5;241m=\u001b[39m MDETR(\n\u001b[1;32m     45\u001b[0m     backbone,\n\u001b[1;32m     46\u001b[0m     transformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     contrastive_hdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask:\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/models/transformer.py:64\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, normalize_before, return_intermediate_dec, pass_pos_and_query, text_encoder_type, freeze_text_encoder, contrastive_loss)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_text_dir, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Fallback to model id; if offline, this will use cache only\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_encoder_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     68\u001b[0m         text_encoder_type, local_files_only\u001b[38;5;241m=\u001b[39moffline\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freeze_text_encoder:\n",
            "File \u001b[0;32m~/anaconda3/envs/reflect_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1809\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1804\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1806\u001b[0m     )\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1810\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1813\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1814\u001b[0m     )\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
          ]
        }
      ],
      "source": [
        "# 设置离线环境变量（优先使用本地权重）\n",
        "# 如果遇到模型加载问题，可以临时设置为 \"0\" 允许从网络下载\n",
        "USE_OFFLINE_MODE = True  # 设置为 False 可以允许从网络下载模型\n",
        "\n",
        "if USE_OFFLINE_MODE:\n",
        "    os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
        "else:\n",
        "    os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"0\")\n",
        "    print(\"⚠️  离线模式已关闭，将允许从网络下载模型（如果缓存中不存在）\")\n",
        "\n",
        "os.environ.setdefault(\"HF_HOME\", f\"{repo_root}/.cache/huggingface\")\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
        "os.environ.setdefault(\"MDETR_TIMM_PRETRAINED\", \"0\")\n",
        "\n",
        "# 配置本地模型路径（如果存在）\n",
        "local_roberta_dir = f\"{repo_root}/models/roberta-base\"\n",
        "if os.path.isdir(local_roberta_dir):\n",
        "    os.environ[\"MDETR_TEXT_ENCODER_DIR\"] = local_roberta_dir\n",
        "    print(f\"✓ 使用本地 Roberta 模型: {local_roberta_dir}\")\n",
        "else:\n",
        "    print(f\"⚠️  本地 Roberta 模型不存在: {local_roberta_dir}\")\n",
        "    print(\"   将尝试使用 HuggingFace 缓存（如果离线模式失败，请先下载模型）\")\n",
        "\n",
        "# 导入 real-world 核心模块（避免导入会立即初始化模型的模块）\n",
        "# 初始化占位符（如果导入失败，这些变量将保持为 None）\n",
        "mdetr_efficientnetB3_phrasecut = None\n",
        "LLMPrompter = None\n",
        "\n",
        "try:\n",
        "    # 从 hubconf 导入函数，而不是从 mdetr_object_detector（避免模块级初始化）\n",
        "    from hubconf import mdetr_efficientnetB3_phrasecut\n",
        "    from real_world_scene_graph import SceneGraph, Node as SceneGraphNode\n",
        "    from real_world_get_local_sg import get_scene_graph\n",
        "    from real_world_utils import get_robot_plan\n",
        "    print(\"✓ real-world 核心模块导入成功（模型将在需要时初始化）\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ 导入错误: {e}\")\n",
        "    print(\"请确保您在正确的目录中运行此 notebook\")\n",
        "\n",
        "# 单独导入 LLMPrompter（尝试多种方式）\n",
        "try:\n",
        "    from LLM.prompt import LLMPrompter\n",
        "    print(\"✓ LLMPrompter 导入成功（方式1：LLM.prompt）\")\n",
        "except ImportError as e1:\n",
        "    try:\n",
        "        # 尝试直接导入\n",
        "        import sys\n",
        "        sys.path.insert(0, os.getcwd())\n",
        "        from LLM.prompt import LLMPrompter\n",
        "        print(\"✓ LLMPrompter 导入成功（方式2：直接导入）\")\n",
        "    except ImportError as e2:\n",
        "        try:\n",
        "            # 尝试使用 importlib\n",
        "            import importlib.util\n",
        "            spec = importlib.util.spec_from_file_location(\"prompt\", \"LLM/prompt.py\")\n",
        "            prompt_module = importlib.util.module_from_spec(spec)\n",
        "            spec.loader.exec_module(prompt_module)\n",
        "            LLMPrompter = prompt_module.LLMPrompter\n",
        "            print(\"✓ LLMPrompter 导入成功（方式3：importlib）\")\n",
        "        except Exception as e3:\n",
        "            print(f\"⚠️  LLMPrompter 导入失败（所有方式都失败）\")\n",
        "            print(f\"   方式1错误: {e1}\")\n",
        "            print(f\"   方式2错误: {e2}\")\n",
        "            print(f\"   方式3错误: {e3}\")\n",
        "            print(\"   后续将使用简单的字符串匹配作为后备\")\n",
        "            LLMPrompter = None\n",
        "\n",
        "# 导入自适应模块\n",
        "sys.path.insert(0, repo_root)\n",
        "from reflect.adaptation import create_adaptive_executor, FailureDatabase, LocalSpecialistModel\n",
        "print(\"✓ 自适应模块导入成功\")\n",
        "\n",
        "# 设置设备\n",
        "torch.set_grad_enabled(False)\n",
        "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"使用设备: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 2: 任务配置和数据加载\n",
        "\n",
        "本步骤将：\n",
        "- 从 `tasks_real_world.json` 加载任务配置\n",
        "- 选择一个任务（例如：`makeCoffee2`）\n",
        "- 加载对应的数据集（zarr 和视频）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已选择任务: 'make coffee'\n",
            "数据文件夹: makeCoffee2\n",
            "预计失败原因: A mug is already inside the coffee machine, as a result, the cup cannot be put inside.\n",
            "目标物体列表: ['coffee machine', 'purple cup', 'blue cup with handle', 'table on the left of sink']\n",
            "动作序列: ['Pick up cup', 'Put cup in coffee machine', 'Toggle on coffee machine', 'Pick up cup', 'Put cup on table']\n",
            "成功条件: a cup filled with coffee is on table.\n"
          ]
        }
      ],
      "source": [
        "# 任务选择 - 您可以修改这个数字来选择不同的任务\n",
        "TASK_ID_TO_RUN = 21  # 示例：选择第21个任务 \"makeCoffee2\"\n",
        "\n",
        "# 加载任务配置\n",
        "with open('tasks_real_world.json', 'r') as f:\n",
        "    tasks_json = json.load(f)\n",
        "\n",
        "# 获取选择的任务信息\n",
        "task_key = f'Task {TASK_ID_TO_RUN}'\n",
        "task_info = tasks_json[task_key]\n",
        "folder_name = task_info[\"general_folder_name\"]\n",
        "\n",
        "print(f\"已选择任务: '{task_info['name']}'\")\n",
        "print(f\"数据文件夹: {folder_name}\")\n",
        "print(f\"预计失败原因: {task_info['gt_failure_reason']}\")\n",
        "print(f\"目标物体列表: {task_info['object_list']}\")\n",
        "print(f\"动作序列: {task_info['actions']}\")\n",
        "print(f\"成功条件: {task_info['success_condition']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据文件路径: /home/fdse/zzy/reflect/reflect_dataset/real_data/makeCoffee2/replay_buffer.zarr\n",
            "RGB视频路径: /home/fdse/zzy/reflect/reflect_dataset/real_data/makeCoffee2/videos/color\n",
            "✓ 数据文件存在\n",
            "  总帧数: 6568\n",
            "✓ RGB视频文件存在，共0帧\n"
          ]
        }
      ],
      "source": [
        "# 检查数据文件是否存在\n",
        "data_path = f'{repo_root}/reflect/reflect_dataset/real_data/{folder_name}/replay_buffer.zarr'\n",
        "video_color_path = f'{repo_root}/reflect/reflect_dataset/real_data/{folder_name}/videos/color'\n",
        "\n",
        "print(f\"数据文件路径: {data_path}\")\n",
        "print(f\"RGB视频路径: {video_color_path}\")\n",
        "\n",
        "# 检查文件是否存在\n",
        "if os.path.exists(data_path):\n",
        "    print(\"✓ 数据文件存在\")\n",
        "    meta_data = zarr.open(data_path, 'r')\n",
        "    total_frames = len(meta_data['data/stage'])\n",
        "    print(f\"  总帧数: {total_frames}\")\n",
        "else:\n",
        "    print(\"✗ 数据文件不存在\")\n",
        "    meta_data = None\n",
        "\n",
        "if os.path.exists(video_color_path):\n",
        "    color_files = sorted([f for f in os.listdir(video_color_path) if f.endswith('.png')])\n",
        "    print(f\"✓ RGB视频文件存在，共{len(color_files)}帧\")\n",
        "else:\n",
        "    print(\"✗ RGB视频文件不存在\")\n",
        "    color_files = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 3: 初始化模型和检测器\n",
        "\n",
        "本步骤将：\n",
        "- 初始化 MDETR 物体检测器（可选）\n",
        "- 初始化 LLM 接口（需要配置 API Key）\n",
        "- 创建输出目录\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "所有输出目录创建完成！\n"
          ]
        }
      ],
      "source": [
        "# 创建输出目录\n",
        "output_dirs = [\n",
        "    f'state_summary/{folder_name}',\n",
        "    f'state_summary/{folder_name}/mdetr_obj_det',\n",
        "    f'state_summary/{folder_name}/mdetr_obj_det/images',\n",
        "    f'state_summary/{folder_name}/mdetr_obj_det/det',\n",
        "    f'state_summary/{folder_name}/mdetr_obj_det/clip_processed_det',\n",
        "    f'state_summary/{folder_name}/local_graphs',\n",
        "    f'scene/{folder_name}'\n",
        "]\n",
        "\n",
        "for dir_path in output_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"所有输出目录创建完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在初始化 MDETR 物体检测器...\n",
            "使用已有检测结果（如果存在）\n"
          ]
        }
      ],
      "source": [
        "# 初始化 MDETR 物体检测器（可选，如果已有检测结果可以跳过）\n",
        "print(\"正在初始化 MDETR 物体检测器...\")\n",
        "detector = None  # 为了演示速度，这里设为 None，使用已有检测结果\n",
        "\n",
        "# 如果需要实时检测，取消下面的注释：\n",
        "# 注意：首次初始化可能需要下载模型或从缓存加载\n",
        "# try:\n",
        "#     if mdetr_efficientnetB3_phrasecut is not None:\n",
        "#         print(\"正在加载 MDETR 模型（可能需要一些时间）...\")\n",
        "#         detector = mdetr_efficientnetB3_phrasecut(pretrained=False).to(device)\n",
        "#         detector.eval()\n",
        "#         print(\"✓ MDETR 检测器初始化成功\")\n",
        "#     else:\n",
        "#         print(\"⚠️  MDETR 函数未导入，无法初始化检测器\")\n",
        "# except Exception as e:\n",
        "#     print(f\"✗ MDETR 检测器初始化失败: {e}\")\n",
        "#     print(f\"   错误详情: {type(e).__name__}: {str(e)}\")\n",
        "#     detector = None\n",
        "\n",
        "print(\"使用已有检测结果（如果存在）\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在初始化 LLM 接口...\n",
            "⚠️  LLMPrompter 未导入，无法初始化 LLM 接口\n",
            "   请检查 Cell 4 中的导入是否成功\n",
            "   或者 LLM 接口将使用简单的字符串匹配作为后备\n"
          ]
        }
      ],
      "source": [
        "# 初始化 LLM 接口（需要配置 API Key）\n",
        "print(\"正在初始化 LLM 接口...\")\n",
        "# 注意：您需要在这里设置您的 OpenAI API Key\n",
        "OPENAI_API_KEY = \"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\" # 请设置您的 API Key\n",
        "\n",
        "llm_prompter = None\n",
        "\n",
        "# 检查 LLMPrompter 是否已导入\n",
        "if 'LLMPrompter' not in globals() or LLMPrompter is None:\n",
        "    print(\"⚠️  LLMPrompter 未导入，无法初始化 LLM 接口\")\n",
        "    print(\"   请检查 Cell 4 中的导入是否成功\")\n",
        "    print(\"   或者 LLM 接口将使用简单的字符串匹配作为后备\")\n",
        "elif OPENAI_API_KEY:\n",
        "    try:\n",
        "        llm_prompter = LLMPrompter(gpt_version=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
        "        print(\"✓ LLM 接口初始化成功\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ LLM 接口初始化失败: {e}\")\n",
        "        llm_prompter = None\n",
        "else:\n",
        "    print(\"⚠️  请设置 OPENAI_API_KEY 后取消注释上面的代码\")\n",
        "    print(\"   或者 LLM 接口将使用简单的字符串匹配作为后备\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 4: 定义适配函数（对接 real-world 模块）\n",
        "\n",
        "本步骤将：\n",
        "- 定义 `get_vllm_description`：使用 real-world 的场景图生成作为 V-LLM 描述\n",
        "- 定义 `reflect_llm_compare`：使用 LLMPrompter 进行 REFLECT 失败检测\n",
        "\n",
        "**这两个函数是连接 real-world 模块和自适应框架的桥梁！**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_hierarchical_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_scene_text\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 全局变量：存储场景图生成所需的参数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m scene_graph_cache \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/real_world_hierarchical_prompt.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmdetr_object_detector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_scene_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node \u001b[38;5;28;01mas\u001b[39;00m SceneGraphNode\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreal_world_scene_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SceneGraph\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/mdetr_object_detector.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model = mdetr_efficientnetB5(pretrained=True).to(device)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m seg_model \u001b[38;5;241m=\u001b[39m \u001b[43mmdetr_efficientnetB3_phrasecut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m seg_model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/hubconf.py:211\u001b[0m, in \u001b[0;36mmdetr_efficientnetB3_phrasecut\u001b[0;34m(pretrained, threshold, return_postprocessor)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmdetr_efficientnetB3_phrasecut\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, return_postprocessor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    MDETR ENB3 with 6 encoder and 6 decoder layers.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Trained on Phrasecut, achieves 53.7 M-IoU on the test set\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_make_detr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimm_tf_efficientnet_b3_ns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_align_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m    213\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m    214\u001b[0m             url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://zenodo.org/record/4721981/files/phrasecut_EB3_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m             map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m             check_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    217\u001b[0m         )\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/hubconf.py:43\u001b[0m, in \u001b[0;36m_make_detr\u001b[0;34m(backbone_name, num_queries, mask, qa_dataset, predict_final, text_encoder, contrastive_align_loss)\u001b[0m\n\u001b[1;32m     41\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     42\u001b[0m backbone \u001b[38;5;241m=\u001b[39m _make_backbone(backbone_name, mask)\n\u001b[0;32m---> 43\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_intermediate_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_encoder_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m detr \u001b[38;5;241m=\u001b[39m MDETR(\n\u001b[1;32m     45\u001b[0m     backbone,\n\u001b[1;32m     46\u001b[0m     transformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     contrastive_hdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask:\n",
            "File \u001b[0;32m~/zzy/reflect/real-world/models/transformer.py:64\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, normalize_before, return_intermediate_dec, pass_pos_and_query, text_encoder_type, freeze_text_encoder, contrastive_loss)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_text_dir, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Fallback to model id; if offline, this will use cache only\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_encoder_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     68\u001b[0m         text_encoder_type, local_files_only\u001b[38;5;241m=\u001b[39moffline\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freeze_text_encoder:\n",
            "File \u001b[0;32m~/anaconda3/envs/reflect_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1809\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1804\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1806\u001b[0m     )\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1810\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1813\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1814\u001b[0m     )\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
          ]
        }
      ],
      "source": [
        "from real_world_hierarchical_prompt import get_scene_text\n",
        "\n",
        "# 全局变量：存储场景图生成所需的参数\n",
        "scene_graph_cache = {}\n",
        "\n",
        "def get_vllm_description(image: Image.Image, step_idx: int = 0) -> str:\n",
        "    \"\"\"\n",
        "    使用 real-world 的场景图生成作为 V-LLM 描述\n",
        "    \n",
        "    参数:\n",
        "        image: PIL Image 对象\n",
        "        step_idx: 步骤索引（用于缓存）\n",
        "    \n",
        "    返回:\n",
        "        场景描述文本（例如：\"coffee machine, purple cup. purple cup is on coffee machine.\"）\n",
        "    \"\"\"\n",
        "    # 简化版本：如果已有场景图，直接使用；否则返回基本描述\n",
        "    cache_key = f\"{folder_name}_{step_idx}\"\n",
        "    \n",
        "    if cache_key in scene_graph_cache:\n",
        "        scene_graph = scene_graph_cache[cache_key]\n",
        "        scene_text = get_scene_text(scene_graph)\n",
        "        return scene_text\n",
        "    \n",
        "    # 如果没有缓存，返回一个基本描述（实际应用中应该调用 get_scene_graph）\n",
        "    # 为了演示，这里返回一个占位描述\n",
        "    return \"coffee machine, purple cup. purple cup is on coffee machine.\"\n",
        "\n",
        "print(\"✓ get_vllm_description 函数定义完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ reflect_llm_compare 函数定义完成\n"
          ]
        }
      ],
      "source": [
        "def reflect_llm_compare(task_command: str, scene_description: str):\n",
        "    \"\"\"\n",
        "    使用 LLMPrompter 进行 REFLECT 失败检测\n",
        "    \n",
        "    参数:\n",
        "        task_command: 任务指令（例如：\"make coffee\"）\n",
        "        scene_description: 场景描述（来自 get_vllm_description）\n",
        "    \n",
        "    返回:\n",
        "        (is_success: bool, failure_reason: str)\n",
        "    \"\"\"\n",
        "    if llm_prompter is None:\n",
        "        # 如果没有 LLM，使用简单的字符串匹配作为后备\n",
        "        success_condition = task_info['success_condition']\n",
        "        if success_condition.lower() in scene_description.lower():\n",
        "            return True, \"\"\n",
        "        return False, f\"Success condition '{success_condition}' not met in scene.\"\n",
        "    \n",
        "    # 使用 LLM 进行推理（实际实现需要根据你的 prompt 格式）\n",
        "    # 这里是一个简化的示例\n",
        "    prompt = {\n",
        "        'system': 'You are a robot failure detection system.',\n",
        "        'user': f\"Task: {task_command}\\nScene: {scene_description}\\nSuccess condition: {task_info['success_condition']}\\n\\nIs the task successful? If not, why?\"\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response, _ = llm_prompter.query(prompt, {'temperature': 0.0}, save=False, save_dir='')\n",
        "        \n",
        "        # 解析响应（简化版本）\n",
        "        if 'success' in response.lower() and 'not' not in response.lower():\n",
        "            return True, \"\"\n",
        "        else:\n",
        "            return False, f\"LLM: {response}\"\n",
        "    except Exception as e:\n",
        "        # 如果 LLM 调用失败，回退到简单匹配\n",
        "        success_condition = task_info['success_condition']\n",
        "        if success_condition.lower() in scene_description.lower():\n",
        "            return True, \"\"\n",
        "        return False, f\"LLM error: {e}\"\n",
        "\n",
        "print(\"✓ reflect_llm_compare 函数定义完成\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 5: 创建自适应执行器\n",
        "\n",
        "本步骤将：\n",
        "- 使用工厂函数创建 `AdaptiveExecutor`\n",
        "- 配置本地专家模型和失败数据库\n",
        "- 准备执行任务\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'create_adaptive_executor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 创建自适应执行器\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m executor \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_adaptive_executor\u001b[49m(\n\u001b[1;32m      3\u001b[0m     get_vllm_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m img: get_vllm_description(img, step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# 包装函数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     reflect_llm_compare\u001b[38;5;241m=\u001b[39mreflect_llm_compare,\n\u001b[1;32m      5\u001b[0m     specialist_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m      6\u001b[0m     db_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(Path(repo_root) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect/adaptation/data\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 获取内部组件（便于演示）\u001b[39;00m\n\u001b[1;32m     10\u001b[0m db: FailureDatabase \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mdb\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_adaptive_executor' is not defined"
          ]
        }
      ],
      "source": [
        "# 创建自适应执行器\n",
        "executor = create_adaptive_executor(\n",
        "    get_vllm_description=lambda img: get_vllm_description(img, step_idx=0),  # 包装函数\n",
        "    reflect_llm_compare=reflect_llm_compare,\n",
        "    specialist_threshold=0.8,\n",
        "    db_root=str(Path(repo_root) / \"reflect/adaptation/data\"),\n",
        ")\n",
        "\n",
        "# 获取内部组件（便于演示）\n",
        "db: FailureDatabase = executor.db\n",
        "sp: LocalSpecialistModel = executor.specialist\n",
        "\n",
        "print(\"✓ 自适应执行器创建完成\")\n",
        "print(f\"  数据库路径: {executor.config.db_root}\")\n",
        "print(f\"  专家模型阈值: {executor.config.specialist_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 6: 加载真实场景图像并执行任务（第一次）\n",
        "\n",
        "本步骤将：\n",
        "- 从真实数据集中加载一张场景图像\n",
        "- 执行任务（预计失败）\n",
        "- 观察失败记录和场景图像保存\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✗ 无法加载图像，请检查 frame_idx 和 color_files\n"
          ]
        }
      ],
      "source": [
        "# 选择一个关键帧（例如：动作执行中的某一帧）\n",
        "frame_idx = 2130  # 可以根据任务调整\n",
        "\n",
        "# 加载图像\n",
        "if color_files and frame_idx < len(color_files):\n",
        "    image_path = os.path.join(video_color_path, color_files[frame_idx])\n",
        "    scene_image = Image.open(image_path).convert('RGB')\n",
        "    print(f\"✓ 加载图像: {image_path}\")\n",
        "    print(f\"  图像尺寸: {scene_image.size}\")\n",
        "    scene_image\n",
        "else:\n",
        "    print(\"✗ 无法加载图像，请检查 frame_idx 和 color_files\")\n",
        "    scene_image = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 执行任务（第一次）\n",
        "if scene_image:\n",
        "    task_command = task_info['name']  # 例如：\"make coffee\"\n",
        "    \n",
        "    print(f\"\\n=== 执行任务: {task_command} ===\")\n",
        "    print(f\"成功条件: {task_info['success_condition']}\")\n",
        "    \n",
        "    first_success = executor.execute_task_with_adaptation(\n",
        "        task_command=task_command,\n",
        "        current_image=scene_image\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n第一次执行结果: {'成功' if first_success else '失败'}\")\n",
        "    \n",
        "    # 检查失败记录\n",
        "    if not first_success:\n",
        "        print(\"\\n失败记录已保存到数据库\")\n",
        "        print(f\"数据库路径: {executor.config.db_root}\")\n",
        "else:\n",
        "    print(\"无法执行任务：场景图像未加载\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 7: 人类纠正并学习\n",
        "\n",
        "本步骤将：\n",
        "- 模拟人类纠正：从场景图像中标注目标物体（例如：\"purple cup\"）\n",
        "- 将纠正样本保存到数据库\n",
        "- 让本地专家模型学习这个纠正样本\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模拟人类纠正：标注目标物体\n",
        "# 在实际系统中，这里会有一个 UI 让用户框选物体\n",
        "\n",
        "if scene_image:\n",
        "    # 选择一个目标物体进行纠正（从 object_list 中选择）\n",
        "    target_object = task_info['object_list'][0] if task_info['object_list'] else \"purple cup\"\n",
        "    \n",
        "    print(f\"\\n=== 人类纠正：标注 '{target_object}' ===\")\n",
        "    \n",
        "    # 方式1：使用整图作为正样本（简化演示）\n",
        "    use_bbox = False  # 设为 True 可以使用 bbox\n",
        "    bbox = (100, 120, 260, 280)  # 示例 bbox，需要根据实际图像调整\n",
        "    \n",
        "    if use_bbox:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        crop_image = scene_image.crop((x1, y1, x2, y2))\n",
        "    else:\n",
        "        crop_image = scene_image.copy()  # 演示用：整图作为正样本\n",
        "    \n",
        "    # 保存纠正到数据库并学习\n",
        "    _ = db.log_correction(\n",
        "        task_command=task_command,\n",
        "        label=target_object,\n",
        "        crop_image=crop_image,\n",
        "        bbox_xyxy=list(bbox) if use_bbox else None\n",
        "    )\n",
        "    \n",
        "    sp.learn_from_correction(crop_image, target_object)\n",
        "    \n",
        "    print(f\"✓ 纠正样本已保存并学习\")\n",
        "    print(f\"  目标物体: {target_object}\")\n",
        "    print(f\"  图像尺寸: {crop_image.size}\")\n",
        "    \n",
        "    # 显示裁剪后的图像\n",
        "    crop_image\n",
        "else:\n",
        "    print(\"无法进行纠正：场景图像未加载\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 8: 再次执行任务（验证学习效果）\n",
        "\n",
        "本步骤将：\n",
        "- 使用相同的场景图像再次执行任务\n",
        "- 观察本地专家模型是否能识别已学习的物体\n",
        "- 验证\"不要在同一地方摔倒两次\"的效果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 再次执行任务\n",
        "if scene_image:\n",
        "    print(f\"\\n=== 再次执行任务: {task_command} ===\")\n",
        "    \n",
        "    second_success = executor.execute_task_with_adaptation(\n",
        "        task_command=task_command,\n",
        "        current_image=scene_image\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n第二次执行结果: {'成功' if second_success else '失败'}\")\n",
        "    \n",
        "    # 对比两次执行结果\n",
        "    print(f\"\\n=== 执行结果对比 ===\")\n",
        "    print(f\"第一次: {'成功' if first_success else '失败'}\")\n",
        "    print(f\"第二次: {'成功' if second_success else '失败'}\")\n",
        "    \n",
        "    if not first_success and second_success:\n",
        "        print(\"\\n🎉 成功！本地专家模型学会了识别目标物体！\")\n",
        "    elif not first_success and not second_success:\n",
        "        print(\"\\n⚠️  仍然失败，可能需要更多纠正样本或调整阈值\")\n",
        "    else:\n",
        "        print(\"\\nℹ️  两次都成功（或都失败），请检查场景描述逻辑\")\n",
        "else:\n",
        "    print(\"无法执行任务：场景图像未加载\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 9: 查看数据库记录\n",
        "\n",
        "本步骤将：\n",
        "- 查看失败记录\n",
        "- 查看纠正记录\n",
        "- 验证数据持久化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 查看失败记录\n",
        "failures_file = Path(executor.config.db_root) / \"failures.jsonl\"\n",
        "if failures_file.exists():\n",
        "    print(f\"\\n=== 失败记录 ({failures_file}) ===\")\n",
        "    with open(failures_file, 'r') as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            record = json.loads(line)\n",
        "            print(f\"\\n失败记录 #{i}:\")\n",
        "            print(f\"  任务: {record.get('task_command', 'N/A')}\")\n",
        "            print(f\"  时间: {record.get('timestamp', 'N/A')}\")\n",
        "            print(f\"  失败原因: {record.get('failure_reason', 'N/A')}\")\n",
        "else:\n",
        "    print(\"\\n暂无失败记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 查看纠正记录\n",
        "corrections_file = Path(executor.config.db_root) / \"corrections.jsonl\"\n",
        "if corrections_file.exists():\n",
        "    print(f\"\\n=== 纠正记录 ({corrections_file}) ===\")\n",
        "    with open(corrections_file, 'r') as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            record = json.loads(line)\n",
        "            print(f\"\\n纠正记录 #{i}:\")\n",
        "            print(f\"  任务: {record.get('task_command', 'N/A')}\")\n",
        "            print(f\"  标签: {record.get('label', 'N/A')}\")\n",
        "            print(f\"  时间: {record.get('timestamp', 'N/A')}\")\n",
        "            print(f\"  图像路径: {record.get('image_path', 'N/A')}\")\n",
        "else:\n",
        "    print(\"\\n暂无纠正记录\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤 10: 可选 - 后台训练器\n",
        "\n",
        "如果你希望系统在后台自动\"看见新纠正就学习\"，可以启动训练器：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from reflect.adaptation import SpecialistTrainer\n",
        "\n",
        "# 创建训练器（可选）\n",
        "trainer = SpecialistTrainer(db, sp, poll_interval_sec=5.0)\n",
        "\n",
        "# 启动后台训练（取消注释以启用）\n",
        "# trainer.start()\n",
        "# ... 运行你的系统 ...\n",
        "# trainer.stop()\n",
        "\n",
        "print(\"训练器对象已准备。取消注释 start()/stop() 以运行后台训练。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 小结\n",
        "\n",
        "本 Notebook 演示了：\n",
        "\n",
        "1. ✅ **环境设置**：加载 real-world 模块和真实数据集\n",
        "2. ✅ **任务配置**：从 `tasks_real_world.json` 加载任务信息\n",
        "3. ✅ **场景图生成**：使用 real-world 的 `get_scene_graph` 作为 V-LLM 描述\n",
        "4. ✅ **REFLECT 失败检测**：使用 LLMPrompter 进行任务成功/失败判定\n",
        "5. ✅ **自适应执行器**：集成本地专家模型和决策融合\n",
        "6. ✅ **失败记录**：自动记录失败场景和原因\n",
        "7. ✅ **人类纠正**：模拟人类标注，让本地专家学习\n",
        "8. ✅ **学习验证**：再次执行任务，验证学习效果\n",
        "\n",
        "### 下一步\n",
        "\n",
        "- 将 `get_vllm_description` 完整实现为调用 `get_scene_graph`\n",
        "- 完善 `reflect_llm_compare` 的 prompt 格式（参考你的 REFLECT prompt）\n",
        "- 集成真实的 UI 进行人类纠正（如 Gradio/Streamlit）\n",
        "- 使用命令行工具提交纠正：\n",
        "  ```bash\n",
        "  python -m reflect.adaptation.tools.submit_correction \\\\\n",
        "    --task \"make coffee\" \\\\\n",
        "    --image /path/to/failure_scene.png \\\\\n",
        "    --label \"purple cup\" \\\\\n",
        "    --bbox 100,120,220,260\n",
        "  ```\n",
        "\n",
        "### 数据持久化\n",
        "\n",
        "- 失败记录：`reflect/adaptation/data/failures.jsonl`\n",
        "- 纠正记录：`reflect/adaptation/data/corrections.jsonl`\n",
        "- 场景图像：`reflect/adaptation/data/images/`\n",
        "- 专家记忆：`reflect/adaptation/data/specialist_memory.pkl`（自动持久化）\n",
        "\n",
        "**最终目标：在你的部署环境中\"持续学习与适应\"，真正做到不在同一处摔倒两次！** 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Reflect Env",
      "language": "python",
      "name": "reflect_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
